{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6xSOnJEVmwcw"
      ],
      "authorship_tag": "ABX9TyNFIZTuj/c3U4K75HoLxWqx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hwismos/capstone-keras-based-model/blob/main/node2vec/node2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경 설정"
      ],
      "metadata": {
        "id": "6xSOnJEVmwcw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqZzuwXe6fB0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/4학년/캡스톤/[인공지능] 실습/[05.08] Keras-node2vec\""
      ],
      "metadata": {
        "id": "R6Ef38sA64A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# node2vec을 이용한 그래프 표상 학습\n",
        "- movielens 데이터셋을 이용해 영화에 대한 임베딩을 생성할 수 있는 node2vec 모델을 구현한다.\n"
      ],
      "metadata": {
        "id": "SRkn78h7nUem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "- 그래프 구조의 객체로부터 유용한 표상을 학습하는 것은 머신러닝 응용에 있어 유용하다. \n",
        "    - 특히, 추천 시스템에 이용될 수 있다.\n",
        "- Graph representation Learning(그래프 표상 학습)은 그래프의 노드들에 대한 임베딩을 학습하는 것을 목표로 한다.\n",
        "    - 노드 라벨 예측(__node classification__)과 링크 예측(__recommendation__)에 이용될 수 있다.\n",
        "- node2vec은 그래프의 노드에 대한 저차원 임베딩을 학습하는 것에 효과적인 방법이다. \n",
        "    - 그래프의 이웃관계 성질을 최적화하는 것을 목적으로 한다.\n",
        "    - 이웃한 노드들의 임베딩을 유사하게 학습시키는 것을 목적으로 한다.\n",
        "- 아이템들이 그래프 구조로 주어진 데이터에 대하여 node2vec은 다음과 같이 동작한다.\n",
        "    1. random walk를 이용해 아이템 순서들을 생성한다.\n",
        "    2. 생성한 아이템 순서들에 대해 양성, 음성 학습 예시들을 생성한다.\n",
        "    3. word2vec 모델을 훈련시켜 아이템들에 대한 임베딩을 학습한다. \n",
        "- Movielens 데이터셋은 영화들을 노드로 취급하고 유저들로부터 유사한 레이팅을 받은 영화 간에 간선을 생성하여 그래프 구조로 표현된다.\n",
        "- 학습된 영화들의 임베딩은 영화 추천에 이용될 수 있다."
      ],
      "metadata": {
        "id": "n-wTGHEfrI4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# networks 라이브러리를 필요로 한다.\n",
        "!pip install networks"
      ],
      "metadata": {
        "id": "ebNLZggMuHSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "s8N8pCtInVyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import networkx as nx\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlretrieve\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "IdvNLP2gnlbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the MovieLens dataset and prepare the data\n",
        "- Movielens 100k 데이터셋은 610 명의 유저와 9,742개의 영화 정보를 갖고 있다. \n",
        "    - 100k개의 간선 정보를 갖고 있다.\n",
        "- 다운로드된 폴더 중 movies.dat과 ratings.dat 데이터 파일만을 이용한다."
      ],
      "metadata": {
        "id": "9WVVTz6in2WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlretrieve(\n",
        "        \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\", \"movielens.zip\"\n",
        ")\n",
        "ZipFile(\"movielens.zip\", \"r\").extractall()"
      ],
      "metadata": {
        "id": "nCexoEwin6N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리를 위해 다운로드한 데이터를 pandas DataFrame에 적재한다.\n",
        "\n",
        "# movies를 DataFrame에 로드한다.\n",
        "movies = pd.read_csv(\"./ml-latest-small/movies.csv\")\n",
        "# \"movieId\" 문자열을 생성한다.\n",
        "movies[\"movieId\"] = movies[\"movieId\"].apply(lambda x: f\"movie_{x}\")\n",
        "\n",
        "# ratings를 DataFrame에 로드한다.\n",
        "ratings = pd.read_csv(\"./ml-latest-small/ratings.csv\")\n",
        "# 'rating'을 float으로 변환한다.\n",
        "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n",
        "# \"movie_id\" 문자열을 생성한다.\n",
        "ratings[\"movieId\"] = ratings[\"movieId\"].apply(lambda x: f\"movie_{x}\")\n",
        "\n",
        "print(\"Movies data shape: \", movies.shape)\n",
        "print(\"Ratings data shape: \", ratings.shape)"
      ],
      "metadata": {
        "id": "2aja3C3avNHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ratings df(DataFrame)와 movies df의 인스턴스 샘플을 점검한다.\n",
        "\n",
        "ratings.head()"
      ],
      "metadata": {
        "id": "1H9fX9DnwSuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies.head()"
      ],
      "metadata": {
        "id": "AcQuYJEWwieo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# movies df에 대한 두 가지 유틸리티 함수를 구현한다.\n",
        "\n",
        "def get_movie_title_by_id(movieId):\n",
        "    return list(movies[movies.movieId == movieId].title)[0]\n",
        "\n",
        "def get_movie_id_by_title(title):\n",
        "    return list(movies[movies.title == title].movieId)[0]"
      ],
      "metadata": {
        "id": "5PTE4R46wo0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct the Movie Graph\n",
        "- 두 영화가 같은 사람으로부터 min_rating 이상의 레이팅을 받았다면, 두 영화 노드 간에 간선을 생성한다.\n",
        "- 간선의 가중치는, 두 영화 간 __pointwise mutual information__에 기반하며 이것은 다음과 같이 계산된다.\n",
        "    - log(xy) - log(x) - log(y) + log(D)\n",
        "        - xy는 얼마나 많은 유저들이 동시에 x와 y 영화에 min_rating 이상으로 레이팅을 했는지에 따라 결정된다.\n",
        "        - x는 얼마나 많은 유저들이 영화 x에 min_rating 이상으로 레이팅 했는지에 따라 결정된다.\n",
        "        - y는 얼마나 많은 유저들이 영화 y에 min_rating 이상으로 레이팅 했는지에 따라 결정된다.\n",
        "        - D는 min_rating 이상의 레이팅 값을 갖는 영화들의 개수이다."
      ],
      "metadata": {
        "id": "6obq-CWnxExo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 1: create the weighted eges between movies."
      ],
      "metadata": {
        "id": "4aYhQSVexHxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_rating = 5\n",
        "pair_frequency = defaultdict(int)  # 디폴트 값이 int인 딕셔너리다.\n",
        "item_frequency = defaultdict(int)\n",
        "\n",
        "# min_rating 이상의 레이팅 값을 갖는 인스턴스들을 필터링 한다.\n",
        "rated_movies = ratings[ratings.rating >= min_rating]\n",
        "# rated_movies.head()\n",
        "\n",
        "# 유저별로 인스턴스를 grouping한다.\n",
        "movies_grouped_by_users = list(rated_movies.groupby(\"userId\"))\n",
        "for group in tqdm(\n",
        "    movies_grouped_by_users,\n",
        "    position=0,\n",
        "    leave=True,\n",
        "    desc=\"Cmpute movie rating frequencies\",\n",
        "):\n",
        "    # 유저에 의해 레이팅된 영화들의 리스트를 가져온다.\n",
        "    current_movies = list(group[1][\"movieId\"])\n",
        "\n",
        "    # 이 부분이 잘 이해가 되지 않는다.\n",
        "    for i in range(len(current_movies)):\n",
        "        item_frequency[current_movies[i]] += 1\n",
        "        for j in range(i+1, len(current_movies)):\n",
        "            x = min(current_movies[i], current_movies[j])\n",
        "            y = max(current_movies[i], current_movies[j])\n",
        "            pair_frequency[(x, y)] += 1"
      ],
      "metadata": {
        "id": "zQmL1Q-Qyrup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: create the graph with the nodes and the edges\n"
      ],
      "metadata": {
        "id": "h3USuWn70_M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 노드 간의 간선의 수를 줄이기 위해, 간선의 가중치가 min_weight 이상인 영화 간의 간선들만을 더한다.\n",
        "\n",
        "min_weight = 10\n",
        "D = math.log(sum(item_frequency.values()))\n",
        "\n",
        "# 무방향 movies 그래프를 생성한다.\n",
        "movies_graph = nx.Graph()\n",
        "# 영화 간 가중화된 간선을 더한다.\n",
        "# 간선을 추가하게 되면 자동적으로 영화 노드들은 추가된다.\n",
        "for pair in tqdm(\n",
        "    pair_frequency, position=0, leave=True, desc=\"Creating the movie graph\"\n",
        "):\n",
        "    x, y = pair\n",
        "    xy_frequency = pair_frequency[pair]\n",
        "    x_frequency = item_frequency[x]\n",
        "    y_frequency = item_frequency[y]\n",
        "    pmi = math.log(xy_frequency) - math.log(x_frequency) - math.log(y_frequency) + D\n",
        "    weight = pmi * xy_frequency\n",
        "    # min_weight 이상의 가중치를 갖는 간선들만을 포함한다.\n",
        "    if weight >= min_weight:\n",
        "        movies_graph.add_edge(x, y, weight=weight)"
      ],
      "metadata": {
        "id": "aRVeNXA51DvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프의 노드와 간선의 수를 출력한다.\n",
        "# 조건에 의해 min_weight 이상의 가중치만을 갖는 간선의 양 끝 노드들만이 추가되었기 때문에 노드들의 개수는 기존의 영화 개수보다 적다.\n",
        "\n",
        "print(\"Total number of graph nodes: \", movies_graph.number_of_nodes())\n",
        "print(\"Total number of graph edges: \", movies_graph.number_of_edges())"
      ],
      "metadata": {
        "id": "BporCRp63qXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프의 노드들의 평균 차수(degree)를 출력한다.\n",
        "degrees = []\n",
        "for node in movies_graph.nodes:\n",
        "    degrees.append(movies_graph.degree[node])\n",
        "\n",
        "print(\"Average node degree: \", round(sum(degrees) / len(degrees), 2))"
      ],
      "metadata": {
        "id": "IVWFvyRT4EDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: create vocabulary and a mapping from tokens to integer indices"
      ],
      "metadata": {
        "id": "Gr1HnL4s4lw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab은 그래프의 노드(movie IDs)들이다.\n",
        "\n",
        "vocabulary = [\"NA\"] + list(movies_graph.nodes)\n",
        "vocabulary_lookup = {token: idx for idx, token in enumerate(vocabulary)}"
      ],
      "metadata": {
        "id": "8k7ZPAoO42om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the biased random walk\n",
        "- random walk는 한 노드로부터 랜덤하게 이웃한 노드로 이동한다.\n",
        "- 만약 한 간선이 편중되었다면, 편중된 정도에 따라 확률적으로 해당 간선으로 이어진 노드에 도달하게 된다. \n",
        "- 이 과정이 num_steps만큼 반복되면 관련된 노드들의 sequence(순서)가 생성되게 된다.\n",
        "- biased random walk는 BFS와 DFS 사이에서 두 개의 매개변수를 이용해 균형을 맞춘다.\n",
        "    1. Return parameter(p): 이 값을 이용해 이전 노드에 대한 재방문 가능성을 조절한다. p값을 높게 설정하면 더 다양한 노드들을 방문하는데 반해, 낮게 설정하면 근처 노드들만을 방문한다. \n",
        "    2. In-out parameter(q): 이 값을 이용해 멀리 이동할 가능성을 조절한다. q 값이 클수록 지역적으로만 노드가 방문하고, 작을수록 멀리 있는 노드에도 방문한다."
      ],
      "metadata": {
        "id": "wPygkr8o5Oma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_step(graph, previous, current, p, q):\n",
        "    neighbors = list(graph.neighbors(current))\n",
        "\n",
        "    weights = []\n",
        "    # p와 q를 이용해 이웃 노드에 대한 간선의 가중치를 조정한다.\n",
        "    for neighbor in neighbors:\n",
        "        if neighbor == previous:\n",
        "            # 이전 노드로 돌아갈 가능성을 조정한다.\n",
        "            weights.append(graph[current][neighbor][\"weight\"] / p)\n",
        "        elif graph.has_edge(neighbor, previous):\n",
        "            # 근처 노드를 방문할 가능성은 간선의 가중치와 같다.\n",
        "            weights.append(graph[current][neighbor][\"weight\"])\n",
        "        else:\n",
        "            # 새로운 노드로 이동할 가능성을 조정한다.\n",
        "            weights.append(graph[current][neighbor][\"weight\"] / q)\n",
        "    \n",
        "    # 각 이웃 노드를 방문할 확률을 계산한다.\n",
        "    weight_sum = sum(weights)\n",
        "    probabilities = [weight / weight_sum for weight in weights]\n",
        "    # 확률적으로 다음으로 방문할 이웃 노드를 고른다.\n",
        "    next = np.random.choice(neighbors, size=1, p=probabilities)[0]\n",
        "    return next\n",
        "\n",
        "def random_walk(graph, num_walks, num_steps, p, q):\n",
        "    walks = []\n",
        "    nodes = list(graph.nodes())\n",
        "    # random walk를 여러 번 반복적으로 실행시킨다.\n",
        "    for walk_iteration in range(num_walks):\n",
        "        random.shuffle(nodes)\n",
        "\n",
        "        for node in tqdm(\n",
        "            nodes,\n",
        "            position=0,\n",
        "            leave=True,\n",
        "            desc=f\"Random walks iteration {walk_iteration + 1} of {num_walks}\",\n",
        "        ):\n",
        "            # 그래프의 랜덤한 한 노드로부터 이동을 시작한다.\n",
        "            walk = [node]\n",
        "            # num_steps만큼 랜덤하게 이동한다.\n",
        "            while len(walk) < num_steps:\n",
        "                current = walk[-1]\n",
        "                previous = walk[-2] if len(walk) > 1 else None\n",
        "                # 다음으로 방문할 노드를 계산한다.\n",
        "                next = next_step(graph, previous, current, p, q)\n",
        "                walk.append(next)\n",
        "            walk = [vocabulary_lookup[token] for token in walk]\n",
        "            walks.append(walk)\n",
        "    return walks"
      ],
      "metadata": {
        "id": "_vYK879I5SPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating traing data using the biased random walk"
      ],
      "metadata": {
        "id": "2uxGTyNMulbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# p와 q를 조정할 수 있다.\n",
        "\n",
        "# return parameter\n",
        "p = 1\n",
        "# in-out parameter\n",
        "q = 1\n",
        "# random walks의 반복 횟수\n",
        "num_walks = 5\n",
        "# random walks의 스텝 수\n",
        "num_steps = 10\n",
        "walks = random_walk(movies_graph, num_walks, num_steps, p, q)\n",
        "\n",
        "print(\"\\n\\nNumber of walks generated: \", len(walks))"
      ],
      "metadata": {
        "id": "sejg_HWTurMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate positive and negative examples\n",
        "- skip-gram 모델을 훈련시키기 위해 생성된 walks를 이용해 positive, negative 훈련 샘플을 생성한다. \n",
        "- 각 샘플은 다음과 같은 특징을 갖는다.\n",
        "    1. target: walk 순서에 있는 한 영화이다.\n",
        "    2. context: walk 순서에 있는 다른 영화이다.\n",
        "    3. weight: target과 context가 walk sequences에서 나타나는 횟수이다.\n",
        "    4. label: 두 영화가 walk sequences 샘플에 있다면 1, 아니면 0으로 라벨링한다."
      ],
      "metadata": {
        "id": "LxY-uiVuvNmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate examples"
      ],
      "metadata": {
        "id": "h8dcTY8nwoXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_examples(sequences, window_size, num_negative_samples, vocabulary_size):\n",
        "    example_weights = defaultdict(int)\n",
        "    # Iterate over all sequences (walks).\n",
        "    for sequence in tqdm(\n",
        "        sequences,\n",
        "        position=0,\n",
        "        leave=True,\n",
        "        desc=f\"Generating postive and negative examples\",\n",
        "    ):\n",
        "        # Generate positive and negative skip-gram pairs for a sequence (walk).\n",
        "        pairs, labels = keras.preprocessing.sequence.skipgrams(\n",
        "            sequence,\n",
        "            vocabulary_size=vocabulary_size,\n",
        "            window_size=window_size,\n",
        "            negative_samples=num_negative_samples,\n",
        "        )\n",
        "        for idx in range(len(pairs)):\n",
        "            pair = pairs[idx]\n",
        "            label = labels[idx]\n",
        "            target, context = min(pair[0], pair[1]), max(pair[0], pair[1])\n",
        "            if target == context:\n",
        "                continue\n",
        "            entry = (target, context, label)\n",
        "            example_weights[entry] += 1\n",
        "\n",
        "    targets, contexts, labels, weights = [], [], [], []\n",
        "    for entry in example_weights:\n",
        "        weight = example_weights[entry]\n",
        "        target, context, label = entry\n",
        "        targets.append(target)\n",
        "        contexts.append(context)\n",
        "        labels.append(label)\n",
        "        weights.append(weight)\n",
        "\n",
        "    return np.array(targets), np.array(contexts), np.array(labels), np.array(weights)\n",
        "\n",
        "\n",
        "num_negative_samples = 4\n",
        "targets, contexts, labels, weights = generate_examples(\n",
        "    sequences=walks,\n",
        "    window_size=num_steps,\n",
        "    num_negative_samples=num_negative_samples,\n",
        "    vocabulary_size=len(vocabulary),\n",
        ")"
      ],
      "metadata": {
        "id": "gxVVy10mwrEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과물의 shape을 출력한다.\n",
        "\n",
        "print(f\"Targets shape: {targets.shape}\")\n",
        "print(f\"Contexts shape: {contexts.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "print(f\"Weights shape: {weights.shape}\")"
      ],
      "metadata": {
        "id": "vS4VxgxOxdVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the data into tf.data.Dataset objects"
      ],
      "metadata": {
        "id": "NcRb_cMZxndg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "\n",
        "\n",
        "def create_dataset(targets, contexts, labels, weights, batch_size):\n",
        "    inputs = {\n",
        "        \"target\": targets,\n",
        "        \"context\": contexts,\n",
        "    }\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels, weights))\n",
        "    dataset = dataset.shuffle(buffer_size=batch_size * 2)\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = create_dataset(\n",
        "    targets=targets,\n",
        "    contexts=contexts,\n",
        "    labels=labels,\n",
        "    weights=weights,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "LxntWdX3xrna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the skip-gram model\n",
        "- skip-gram 모델은 심플한 이진 분류 모델이며 다음과 같이 동작한다.\n",
        "    1. target 영화에 대한 임베딩이 검색된다.\n",
        "    2. context 영화에 대한 임베딩이 검색된다.\n",
        "    3. 두 임베딩 간의 내적 연산이 실행된다.\n",
        "    4. label과 시그모이드 활성화 함수를 거친 두 임베딩의 내적 값이 비교된다.\n",
        "    5. binary corssentropy loss가 사용된다."
      ],
      "metadata": {
        "id": "8l-8Zyx1yFtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "embedding_dim = 50\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "_aqLhIU-ygoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the model"
      ],
      "metadata": {
        "id": "KJIV2bb4yisb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocabulary_size, embedding_dim):\n",
        "\n",
        "    inputs = {\n",
        "        \"target\": layers.Input(name=\"target\", shape=(), dtype=\"int32\"),\n",
        "        \"context\": layers.Input(name=\"context\", shape=(), dtype=\"int32\"),\n",
        "    }\n",
        "    # 아이템 임베딩을 초기화한다.\n",
        "    # 양의 정수(인덱스)들을 고정된 사이즈의 dense vector로 치환한다.\n",
        "    embed_item = layers.Embedding(\n",
        "        input_dim=vocabulary_size,\n",
        "        output_dim=embedding_dim,\n",
        "        embeddings_initializer=\"he_normal\",\n",
        "        embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
        "        name=\"item_embeddings\",\n",
        "    )\n",
        "    # 타겟에 대한 임베딩을 검색한다.\n",
        "    target_embeddings = embed_item(inputs[\"target\"])\n",
        "    # context에 대한 임베딩을 검색한다. \n",
        "    context_embeddings = embed_item(inputs[\"context\"])\n",
        "    # target과 context 간의 내적 값을 계산한다.\n",
        "    logits = layers.Dot(axes=1, normalize=False, name=\"dot_similarity\")(\n",
        "        [target_embeddings, context_embeddings]\n",
        "    )\n",
        "    # 모델 인스턴스를 생성한다.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "metadata": {
        "id": "E1tub9DeylPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "pfGBmNuO0rEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 인스턴스를 만든 뒤 컴파일 한다.\n",
        "\n",
        "model = create_model(len(vocabulary), embedding_dim)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")"
      ],
      "metadata": {
        "id": "R0IFY8vB0tf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model을 도식화하여 출력한다.\n",
        "keras.utils.plot_model(\n",
        "    model,\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xSJel6-T02Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에 대해 모델을 훈련시킨다.\n",
        "\n",
        "history = model.fit(dataset, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "uqJQDv4-1Ryh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습에 대한 히스토리를 그래프로 표현한다.\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.ylabel(\"losss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PiDpwqPN1awD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze the learnt embeddings"
      ],
      "metadata": {
        "id": "QBInZhr81ztC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_embeddings = model.get_layer(\"item_embeddings\").get_weights()[0]\n",
        "\n",
        "# 저장된 SavedModel 파일을 이용해 모델을 로드하고 기존과 동일한 결과를 반환하는지를 확인한다.\n",
        "# movie_embeddings = new_model.get_layer(\"item_embeddings\").get_weights()[0]  \n",
        "\n",
        "print(\"Embeddings shape:\", movie_embeddings.shape)"
      ],
      "metadata": {
        "id": "V7sqpei213ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find related movies"
      ],
      "metadata": {
        "id": "Pobm4N9Y3ReX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 몇가지 영화들로 구성된 query_movies 리스트를 정의한다.\n",
        "\n",
        "query_movies = [\n",
        "    \"Matrix, The (1999)\",\n",
        "    \"Star Wars: Episode IV - A New Hope (1977)\",\n",
        "    \"Lion King, The (1994)\",\n",
        "    \"Terminator 2: Judgment Day (1991)\",\n",
        "    \"Godfather, The (1972)\",\n",
        "]"
      ],
      "metadata": {
        "id": "nJOwk4KI3Yl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query_movies 리스트의 영화들에 대한 임베딩을 가져 온다.\n",
        "\n",
        "query_embeddings = []\n",
        "\n",
        "for movie_title in query_movies:\n",
        "    movieId = get_movie_id_by_title(movie_title)\n",
        "    token_id = vocabulary_lookup[movieId]\n",
        "    movie_embedding = movie_embeddings[token_id]\n",
        "    query_embeddings.append(movie_embedding)\n",
        "\n",
        "query_embeddings = np.array(query_embeddings)\n",
        "# print(query_embeddings)"
      ],
      "metadata": {
        "id": "X9uBFom13qO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query_movies의 영화들의 임베딩과 다른 모든 영화 간의 임베딩 간의 코사인 similarity를 계산한다. \n",
        "# 이 중, 유사도가 가장 높은 k를 구한다.\n",
        "\n",
        "# Module: tf.linalg\n",
        "# 선형대수학적 연산을 수행해주는 라이브러리다.\n",
        "similarities = tf.linalg.matmul(\n",
        "    tf.math.l2_normalize(query_embeddings),\n",
        "    tf.math.l2_normalize(movie_embeddings),\n",
        "    transpose_b=True,\n",
        ")\n",
        "\n",
        "_, indices = tf.math.top_k(similarities, k=5)\n",
        "indices = indices.numpy().tolist()"
      ],
      "metadata": {
        "id": "kb78o-md4t-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query_movies 안에 있는 영화들과 가장 높은 연관성을 갖는 영화들을 출력한다.\n",
        "\n",
        "for idx, title in enumerate(query_movies):\n",
        "    print(title)\n",
        "    print(\"\".rjust(len(title), \"-\"))\n",
        "    similar_tokens = indices[idx]\n",
        "    for token in similar_tokens:\n",
        "        similar_movieId = vocabulary[token]\n",
        "        similar_title = get_movie_title_by_id(similar_movieId)\n",
        "        print(f\"- {similar_title}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "0rSn9dYw5Uut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the embeddings using the Embedding Projector"
      ],
      "metadata": {
        "id": "DZ7rqvTK56sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "# tsv 파일은 tab으로 컬럼을 분리한다.comma로 컬럼을 분리하는 csv와 유사하다.\n",
        "out_v = io.open(\"embeddings.tsv\", \"w\", encoding=\"utf-8\")\n",
        "out_m = io.open(\"metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "for idx, movie_id in enumerate(vocabulary[1:]):\n",
        "    movie_title = list(movies[movies.movieId == movie_id].title)[0]\n",
        "    vector = movie_embeddings[idx]\n",
        "    out_v.write(\"\\t\".join([str(x) for x in vector]) + \"\\n\")\n",
        "    out_m.write(movie_title + \"\\n\")\n",
        "\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "metadata": {
        "id": "4suLG0xX6FSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Load"
      ],
      "metadata": {
        "id": "1tYFRp0I7AHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.saving.save_model(\n",
        "    model, './my_model', overwrite=True, save_format=None\n",
        ")"
      ],
      "metadata": {
        "id": "1Ury9JOd7CeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('./my_model')\n",
        "\n",
        "# new_model의 구조가 기존 모델과 동일함을 확인한다.\n",
        "keras.utils.plot_model(\n",
        "    new_model,\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        ")"
      ],
      "metadata": {
        "id": "mbCE7I9w9FKS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}